= Лекция 4. 4 марта 2025.
// Лекция обещает быть весёлой)))её
== Независимость событий ( заголовок на следующую лекцию )

Пусть $(Omega, cal(F), P)$ --- вероятностное пространство.

*Определение*. Случайные события $A$ и $B$ называются *независимыми*, если $P(A sect B) = P(A) P(B)$. // разве не sect?

Таким образом, знание вероятностей по отдельности не позволяет вычислить вероятность их произведения.



=== Теоремы которые помогают вычислить вероятностть их произведения

_Нам нужно отключить жизненное понимание независимости, потому что оно не вполне совпадает с математическим._

Пусть $A, B in cal(F) $ причем $P(B) >0$

События $A$ и $B$ независимы $ <=> P(A | B) = P(A)$ ("$P$ от $A$ при условии $B$ равно $P$ от $A$")

// если мы хотим оргию из 3+ человек, то, видимо, нужно менять плагин
// думаю он на чью-то строку залез и из-за этого отъебнуло (посмотрим, мб реально плагин пиздит)

О больном. Вероятность вес, если мы едим булки? Вероятность набрать вес при поедании булок и вероятность набрать вес при поедании булок со знанием о конкретной начинке (варенье) одинаковы.


*Доказательство:*

// NOTE: Агафонова не хочет слушать про свои булки

Необх.: Пусть $A, B$ независимы.

Тогда $ P(A | B) = (P(A sect B))/(P(B)) = (P(A) dot P(B))/(P(B)) = P(A) $

Пусть верно, что $P(A | B) = P(A)$. Мы знаем, что это верно, но пока не знаем ничего о (не)зависимости событий. Поэтому по теореме умножения вероятностей

// NOTE: мы имеем теорму


Тогда $ P(A sect B) = P(B) dot P(A | B) = P(B) dot P(A) => #[выполнено опредееление \ независимости собтыий] $


=== Теорема (о независимости противоположных событий)

Пусть $A, B$ --- независимые случайные события. Тогда независимы в парах события $A$ и $overline(B)$, $overline(A)$ и $B$, $overline(A)$ и $overline(B)$.

*Доказательство*:

Докажем, что $A$ и $overline(B)$ независимы.

// в конспекте Сани нумерация сбилась на единицу, у него 6, у нас 7
#image("imgs/007.png")

$
  P(A sect overline(B)) = P(A \\ (A B)) = P(A) - P(A sect B) =\
  = P(A) - P(A) dot P(B) = P(A) (1 - P(B)) = P(A) dot P(overline(B))
$

$ overline(A) and overline(B) #[ --- самостаятельно] $

Рассмотрим ещё одно определение, связанное с независимостю. Дело в том, что события наступают не в парах.

/ Опр: Случайные события $A_1, A_2, dots, A_n$ называются независимы в совокупности, если $forall 2 lt.eq k lt.eq n$, $P (limits(sect)_(j = 1)^k A_(i j)) = limits(sect.sq)_(j = 1)^k P(A_(i j))$

// NOTE: A_(i j) или A_(j)

Из совокупности следует попарная независимость, но попарное неверно.

Для иллюстрации этого понятия приведём пример. Если говорить про бытовые вещи, то обсудим следующую ситуацию. У человека 5 по математике и по физкультуре. Вроде события независимые, но если сюда добавить знание биологии, то мы повышаем вероятность того, что это отличник.

// HACK: 
// вот они, истории про чурок пошли
// АРИЙСКИЕ ТОВАРИЩИ ЕБАТЬ
// Теперь абстракции в качестве уважения

Пример Сергея Николаевича Бернштейна /* его много_члены могли появляться в матане */:

Подбрасывается приамидка (тетраэдер):
+ грань красная
+ грань зеленая
+ грань синяя
+ содержащий 3 цвета

#image("imgs/008.png")

Покажем, что события К, С, З --- оппарно независимы, но зависимы в совокупности. К --- грань содержит красный цвет и т.д.

Пусть $k = 2$

$ P(K) = P(C) = P("З") = 2/4 = 1/2 $
$ P(K sect C) = P(K sect "З") = P(C sect "З") = 1/4 $

Для каждой пары
$ underbrace(1/2, P(A)) dot underbrace(1/2, P(B)) = underbrace(1/4, P(A sect B)) #[ это верно] => #[ независимы попарно] $

$k = 3$

$ P(K sect C sect "З") = 1/4 $

$ P(K) dot P(C) dot P("З") = 1/2 dot 1/2 dot 1/2 = 1/8 $

$ 1/4 eq.not 1/8 => #[зависимаы] $

Пример учить не нужно, но он имеет смысл

== Теорема (Формула полной вероятности)
Пусть ${A_i}^infinity_(i = 1)$ --- полная группа попарно несовместимых событий

$ limits(union.big.sq)_(i = 1)^infinity A_i = Omega; space P(A_i) > 0 space forall i $

Пусть $A$ --- случайное событие для которого

$ P(A| A_i) gt.eq 0 $

Тогда

$ P(A) = limits(sum)_(i = 1)^infinity P(A_i) dot P(A \\ A_i) $

_У нас есть элементы $a_1, dots, a_i$. $a_i_1, dots, a_i_k$ --- это перестановка из элементов. Это алгебраическое соглашение об обозначениях, не более. В расшифровке не нуждается._

Это означает, что $a_(i 1), a_(i 2) dots a_(i k)$

_КНиИТ сильнее мехмата на втором курсе, потому что КНиИТ умнее. А потом мы становимся умстенно отсталыми, потому что у нас не хватает математики, а мозги остаются программистскими и математически некультурными._

_$i$ --- мнимая единица, а не счётчик цикла_

*Доказательство*

#image("imgs/009.png")
#image("imgs/010.png")

Мы нарисовали события:

Мы измеряем событие $A$...

Представим $A$ как

$
  P(A) = P(A sect Omega) = P(A sect (limits(union.sq)_(i = 1)^infinity A_i))=\
  = P(limits(union.sq)_(i = 1)^infinity A sect A_i) = limits(sum)_(i = 1)^infinity P(A sect A_i) =\
  = limits(sum)_(i = 1)^infinity P(A_i) dot P(A | A_i)
$

Рассмотрим следующий вопрос. Допустим, нам нужна ситуация, когда много экзаменаторов. Мы сдаём собеседование и вообще не знаем преподавателей. Гипотеза о попадании к каждому преподавателю равновозможна и по умолчанию есть равная вероятность, что конкретный из преподавателей добрый. Вопрос к уже сдавшему студенту "Кому ты сдавал? Каков он?" не влияет на фактическую вероятность того, что преподаватель добрый, но наша оценка конкретного преподавателя изменяется. Это связано с теоремой Байеса.

== Теорема Байеса

Пусть ${A_i}_(i = 1)^infinity$ такое, чтоо $limits(union.big.sq)_(i = 1)^infinity A_i = Omega; P(A_i) > 0$ и для $A_i in cal(F)$ п


Тогда $ P(A_i | A) = (P(A_i) dot P(A | A_i))/(P(A)) $

*Док-во:*
$
	cases(
		P(A sect A_i) = P(A_i) dot P(A | A_i),
		P(A sect A_i) = P(A) dot P(A_i | A)
	)
$

_Мы могли где-то слышать про Байесовскую теорию вероятности._

$A_i$ --- гипотеза, $P(A_i)$ --- априорные вероятности гипотез (как факт принемаем до опыта). $P(A_i | A)$ --- апостериорные вероятности гипотез.

_Байесовский подход к изменению среды_: препод хочет узнать, умный ли студент, на экзамене. Для этого он задаёт вопросы и таким образом испытывает среду: с каждым ответом на вопрос вероятность меняется. Если студент отвечает на сложный вопрос, мы уменьшаем вероятность того, что он троечник, и увеличиваем шанс отличника. Этот метод зародился в геологии, где подобными экспериментами определяли залегающие в недрах металлы.

*С классической теор. вер закончили*

=== Задача 1.
Пусть в урне находятся некоторое кол-во шаров разных цветов (могут быть разных цветов). Шары не отличимы на на ощупь и тп. В урну опустили белый шар, а затем извлекли один шар, оказавшийся белым. Найти вероятность того, что в урне остались белые шары.
// HACK: белые и не белые шары))))

1. Гипотеза
- $A_0$ --- в урне не было белых шаров
- $A_1$ --- в урне бы 1 белый шар
- $dots$
- $A_n$ --- все шары были белыми

Определим вероятности гипотез.

$ P(A_i) = 1/n , #[поскольку нет уточнений относительно цветовых пропорций] $

2. Событие $A$ --- извлечен 1 белый шар

$
	P(A | A_0 ) = 1/(n + 1), \
	P(A | A_1) = 2/(n + 1), \ 
  dots\
  P(A | A_n) = (n + 1)/(n + 1)
$

3.
$ P(A) = limits(sum)_(i = 0)^n P(A_i) dot P(A | A_i) = limits(sum)_(i = 0)^n 1/(n+1) dot (i+1)/(n+1) = 1/(n+1)^2 limits(sum)_(i = 0)^n (i + 1) = (1 + n + 1)/(2(n + 1)^2) = (n+ 2)/(2(n+1)) $

// HACK: Ряд корана
// вот cumень прочитает в след году и пизда тебе

4. Остались ли белые? // HACK: после BLM-а

$B$ --- остались белые

$overline(B)$ --- не остались белые

$P(A_0 | A) = P(overline(B))$

$P(A_0 | A) = (1/(n + 1) dot 1/(n + 1)) / ((n + 2)/(2(n + 1))) = frac(2, (n+2)(n+1))$

// HACK: почему все шары в белом

$P(B) = 1 - 2/((n + 1)(n + 2)) $

$P(B) = limits(sum)_(i = 1)^n = P(A_i | A)$

// HACK: почему шары в белом, а ящик черный?
// всё хуже и хуже
// наши исходники --- это сборник анекдотов чисто







































































































































































